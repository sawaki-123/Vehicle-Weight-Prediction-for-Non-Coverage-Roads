# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dHUpbX8HX3ifHnUI2Px_7zpLTvLZPM4A
"""

import os
print(os.listdir("/content"))

import zipfile


with zipfile.ZipFile("/content/Newyork.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/Newyork")


base_path = "/content/Newyork"

import os

print(os.listdir("/content/Newyork"))

base_path = "/content/Newyork/Newyork"

import os


print("Months found:", os.listdir(base_path))

print(os.listdir(os.path.join(base_path, "june")))

import os

print("Inside base_path:", os.listdir(base_path))
print("Inside june:", os.listdir(os.path.join(base_path, "june")))

print("Columns found in combined data:")
print(combined_df.columns.tolist())

final_cols = ['year', 'month', 'Month', 'day', 'hour', 'lane', 'class', 'speed', 'num_axles', 'gvw']

combined_df = combined_df[final_cols]

print(" Final column order:", combined_df.columns.tolist())

final_cols = ['year', 'month', 'Month', 'day', 'hour', 'lane', 'class', 'speed', 'num_axles', 'gvw']


combined_df = combined_df[final_cols]


print( Final columns:", combined_df.columns.tolist())
print("Shape:", combined_df.shape)


csv_path = "/content/NewYork_Combined_Cleaned.csv"
combined_df.to_csv(csv_path, index=False)

print(f Master file saved as {csv_path}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


csv_path = "/content/NewYork_Combined_Cleaned.csv"
df = pd.read_csv(csv_path)

print(" Data loaded:", df.shape)
print(df.head())


corr = df.corr(numeric_only=True)


corr.to_csv("/content/NewYork_Correlation_Matrix.csv")
print(" Correlation matrix saved as /content/NewYork_Correlation_Matrix.csv")

plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap (NewYork Data)")
plt.show()


features = ['speed', 'class', 'num_axles']

for feat in features:
    plt.figure(figsize=(6,4))
    sns.scatterplot(x=df[feat], y=df['gvw'], alpha=0.3)
    plt.title(f"GVW vs {feat}")
    plt.xlabel(feat)
    plt.ylabel("GVW")
    plt.show()

from google.colab import files

files.download("/content/NewYork_Combined_Cleaned.csv")

month_order = {
    'january': 1, 'february': 2, 'march': 3, 'april': 4,
    'may': 5, 'june': 6, 'july': 7, 'august': 8,
    'september': 9, 'october': 10, 'november': 11, 'december': 12
}


combined_df["Month"] = combined_df["Month"].str.lower()


combined_df["month_num"] = combined_df["Month"].map(month_order)

combined_df["lane"] = combined_df["lane"].astype(str)
combined_df["lane_num"] = combined_df["lane"].str.extract(r'(\d+)').astype(int)

month_order = {
    'january': 1, 'february': 2, 'march': 3, 'april': 4,
    'may': 5, 'june': 6, 'july': 7, 'august': 8,
    'september': 9, 'october': 10, 'november': 11, 'december': 12
}


combined_df["Month"] = combined_df["Month"].astype(str).str.lower()


combined_df["month_num"] = combined_df["Month"].map(month_order)


combined_df["lane"] = combined_df["lane"].astype(str)
combined_df["lane_num"] = combined_df["lane"].str.extract(r'(\d+)').astype(int)


combined_df = combined_df.sort_values(by=["year", "month_num", "lane_num"])


combined_df = combined_df.drop(columns=["month_num", "lane_num"])


combined_df.to_csv("/content/NewYork_Combined_Ordered.csv", index=False)
print("‚úÖ Saved as /content/NewYork_Combined_Ordered.csv")

combined_df.to_csv("/content/NewYork_Combined_Ordered.csv", index=False)
print("‚úÖ Saved as /content/NewYork_Combined_Ordered.csv")

from google.colab import files
files.download("/content/NewYork_Combined_Ordered.csv")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


file_path = "/content/NewYork_Combined_Ordered.csv"
df = pd.read_csv(file_path)

print("‚úÖ Data loaded:", df.shape)


lanes = sorted(df["lane"].astype(str).unique())

for lane in lanes:
    print(f"\n==============================")
    print(f"üîé Lane: {lane}")
    print(f"==============================")


    df_lane = df[df["lane"].astype(str) == lane]
    print("Subset shape:", df_lane.shape)


    corr = df_lane.corr(numeric_only=True)


    corr_path = f"/content/NewYork_Correlation_{lane}.csv"
    corr.to_csv(corr_path)
    print(f"‚úÖ Correlation matrix saved for {lane}: {corr_path}")

    plt.figure(figsize=(10,6))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Correlation Heatmap - NewYork {lane}")
    plt.show()

    target = "gvw"
    if target in corr.columns:
        top_corr = corr[target].abs().sort_values(ascending=False)
        print(f"\nüîé Top correlations with GVW in {lane}:")
        print(top_corr)
    else:
        print("‚ö†Ô∏è GVW column not found in correlations!")

    features = ["class", "num_axles", "speed", "hour"]

    for feat in features:
        if feat in df_lane.columns:
            plt.figure(figsize=(6,4))
            sns.scatterplot(x=df_lane[feat], y=df_lane["gvw"], alpha=0.3)
            plt.title(f"GVW vs {feat} ({lane})")
            plt.xlabel(feat)
            plt.ylabel("GVW")
            plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


file_path = "/content/NewYork_Combined_Ordered.csv"
df = pd.read_csv(file_path)

print("‚úÖ Data loaded:", df.shape)


lanes = sorted(df["lane"].astype(str).unique())  "

for lane in lanes:
    print(f"\n==============================")
    print(f"üîé Lane: {lane}")
    print(f"==============================")


    df_lane = df[df["lane"].astype(str) == lane][["hour", "class", "speed", "num_axles", "gvw"]]
    print("Subset shape:", df_lane.shape)


    corr = df_lane.corr(numeric_only=True)


    corr_path = f"/content/NewYork_Correlation_{lane}_focused.csv"
    corr.to_csv(corr_path)
    print(f"‚úÖ Focused correlation matrix saved for {lane}: {corr_path}")


    plt.figure(figsize=(6,5))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", cbar=True)
    plt.title(f"Correlation Heatmap (Focused) - {lane}")
    plt.show()


    top_corr = corr["gvw"].abs().sort_values(ascending=False)
    print(f"\nüîé Correlations with GVW in {lane}:")
    print(top_corr)


    features = ["class", "num_axles", "speed", "hour"]

    for feat in features:
        plt.figure(figsize=(6,4))
        sns.scatterplot(x=df_lane[feat], y=df_lane["gvw"], alpha=0.3)
        plt.title(f"GVW vs {feat} ({lane})")
        plt.xlabel(feat)
        plt.ylabel("GVW")
        plt.show()


    plt.figure(figsize=(12,6))
    sns.boxplot(x="hour", y="gvw", data=df_lane)
    plt.title(f"GVW Distribution by Hour - {lane}")
    plt.xlabel("Hour of Day")
    plt.ylabel("GVW")
    plt.xticks(rotation=45)
    plt.show()







import os
print(os.listdir("/content"))

df["lane"] = df["lane"].astype(str).str.replace(".0", "", regex=False)

import os
print(os.listdir("/content"))

df["lane"] = df["lane"].astype(str).str.replace(".0", "", regex=False)  s
df["lane"] = "lane_" + df["lane"].astype(str)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os, shutil
from google.colab import files

file_path = "/content/NewYork_Combined_Ordered.csv"
df = pd.read_csv(file_path)

print(" Data loaded:", df.shape)


df["lane"] = df["lane"].astype(str).str.replace(".0", "", regex=False)
df["lane"] = "lane_" + df["lane"].astype(str)


results_dir = "/content/NewYork_results"
os.makedirs(results_dir, exist_ok=True)


lanes = sorted(df["lane"].unique())
for lane in lanes:
    print(f"\nüîé Processing {lane}...")

    df_lane = df[df["lane"] == lane][["hour", "class", "speed", "num_axles", "gvw"]]

    corr = df_lane.corr(numeric_only=True)
    corr_path = os.path.join(results_dir, f"corr_{lane}.xlsx")
    corr.to_excel(corr_path)
    print(f"   üìÑ Correlation saved: {corr_path}")


    plt.figure(figsize=(6,5))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Correlation Heatmap - {lane}")
    heatmap_path = os.path.join(results_dir, f"heatmap_{lane}.png")
    plt.savefig(heatmap_path, dpi=300, bbox_inches="tight")
    plt.close()


    for feat in ["class", "num_axles", "speed", "hour"]:
        plt.figure(figsize=(6,4))
        sns.scatterplot(x=df_lane[feat], y=df_lane["gvw"], alpha=0.3)
        plt.title(f"GVW vs {feat} ({lane})")
        plt.xlabel(feat)
        plt.ylabel("GVW")
        scatter_path = os.path.join(results_dir, f"scatter_{lane}_{feat}.png")
        plt.savefig(scatter_path, dpi=300, bbox_inches="tight")
        plt.close()

    plt.figure(figsize=(12,6))
    sns.boxplot(x="hour", y="gvw", data=df_lane)
    plt.title(f"GVW Distribution by Hour - {lane}")
    plt.xlabel("Hour of Day")
    plt.ylabel("GVW")
    plt.xticks(rotation=45)
    boxplot_path = os.path.join(results_dir, f"boxplot_hour_{lane}.png")
    plt.savefig(boxplot_path, dpi=300, bbox_inches="tight")
    plt.close()



zip_path = "/content/NewYork_results.zip"
shutil.make_archive(zip_path.replace(".zip",""), 'zip', results_dir)


files.download(zip_path)

from google.colab import files
uploaded = files.upload()

import zipfile, os, sys


zip_path = "/content/California.zip"
extract_to = "/content/California"
with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall(extract_to)

print("‚úÖ Unzipped. Top-level in /content/California:", os.listdir(extract_to))


MONTHS_SET = {'january','february','march','april','may','june',
              'july','august','september','october','november','december'}

def find_base_with_months(start_dir):

    lower_dirs = [d.lower() for d in os.listdir(start_dir) if os.path.isdir(os.path.join(start_dir,d))]
    if any(m in lower_dirs for m in MONTHS_SET):
        return start_dir

    for d in os.listdir(start_dir):
        p = os.path.join(start_dir, d)
        if os.path.isdir(p):
            lower_dirs = [x.lower() for x in os.listdir(p) if os.path.isdir(os.path.join(p,x))]
            if any(m in lower_dirs for m in MONTHS_SET):
                return p
    return None

base_path = find_base_with_months(extract_to)
if not base_path:
    p
print("üìÅ Using base_path:", base_path)
print("üìÇ Found in base_path:", os.listdir(base_path))

sample_month = 'june'
print("Lanes in", sample_month, ":", os.listdir(os.path.join(base_path, sample_month)))

import pandas as pd


month_order = ["january","february","march","april","may","june",
               "july","august","september","october","november","december"]

all_frames = []
file_count = 0

for month in month_order:
    month_path = os.path.join(base_path, month)
    if not os.path.exists(month_path):
        continue

    for lane_folder in sorted(os.listdir(month_path)):
        lane_path = os.path.join(month_path, lane_folder)
        if not os.path.isdir(lane_path):
            continue

        for fname in os.listdir(lane_path):
            fpath = os.path.join(lane_path, fname)
            if fname.lower().endswith((".xlsx", ".xls")):
                df = pd.read_excel(fpath)
            elif fname.lower().endswith(".csv"):
                df = pd.read_csv(fpath)
            else:
                continue


            df["Month"] = month


            all_frames.append(df)
            file_count += 1

print(f"üì¶ Files read: {file_count}")
if not all_frames:
    raise RuntimeError("No files were read. Check extensions or folder structure.")

ca_raw = pd.concat(all_frames, ignore_index=True)
print("üßÆ Raw combined shape:", ca_raw.shape)
print(ca_raw.head())

from google.colab import files
uploaded = files.upload()

import zipfile, os, sys


zip_path = "/content/California.zip"
extract_to = "/content/California"
with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall(extract_to)

print("‚úÖ Unzipped. Top-level in /content/California:", os.listdir(extract_to))


MONTHS_SET = {'january','february','march','april','may','june',
              'july','august','september','october','november','december'}

def find_base_with_months(start_dir):

    lower_dirs = [d.lower() for d in os.listdir(start_dir) if os.path.isdir(os.path.join(start_dir,d))]
    if any(m in lower_dirs for m in MONTHS_SET):
        return start_dir

    for d in os.listdir(start_dir):
        p = os.path.join(start_dir, d)
        if os.path.isdir(p):
            lower_dirs = [x.lower() for x in os.listdir(p) if os.path.isdir(os.path.join(p,x))]
            if any(m in lower_dirs for m in MONTHS_SET):
                return p
    return None

base_path = find_base_with_months(extract_to)
if not base_path:

print("üìÅ Using base_path:", base_path)
print("üìÇ Found in base_path:", os.listdir(base_path))

sample_month = 'june'
print("Lanes in", sample_month, ":", os.listdir(os.path.join(base_path, sample_month)))

import pandas as pd


month_order = ["january","february","march","april","may","june",
               "july","august","september","october","november","december"]

all_frames = []
file_count = 0

for month in month_order:
    month_path = os.path.join(base_path, month)
    if not os.path.exists(month_path):
        continue

    for lane_folder in sorted(os.listdir(month_path)):
        lane_path = os.path.join(month_path, lane_folder)
        if not os.path.isdir(lane_path):
            continue

        for fname in os.listdir(lane_path):
            fpath = os.path.join(lane_path, fname)
            if fname.lower().endswith((".xlsx", ".xls")):
                df = pd.read_excel(fpath)
            elif fname.lower().endswith(".csv"):
                df = pd.read_csv(fpath)
            else:
                continue

from google.colab import files
uploaded = files.upload()

import os, sys, zipfile, numpy as np, pandas as pd
from google.colab import files


zip_path = "/content/California.zip"
extract_to = "/content/California"

if not os.path.exists(zip_path):
    raise FileNotFoundError("California.zip not found in /content. Re-run the upload cell.")

with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall(extract_to)

print("‚úÖ Unzipped to:", extract_to)
print("Top-level contents:", os.listdir(extract_to))


MONTHS_LOWER = ['january','february','march','april','may','june',
                'july','august','september','october','november','december']

def has_months(folder):
    try:
        names = [d.lower() for d in os.listdir(folder)
                 if os.path.isdir(os.path.join(folder, d))]
    except Exception:
        return False
    return any(m in names for m in MONTHS_LOWER)

base_path = None

if has_months(extract_to):
    base_path = extract_to
else:

    for d in os.listdir(extract_to):
        p = os.path.join(extract_to, d)
        if os.path.isdir(p) and has_months(p):
            base_path = p
            break

if base_path is None:
    print("‚ùå Could not find month folders (january..december). Check your zip structure.")
    sys.exit(1)

print("üìÅ Using base_path:", base_path)
print("üìÇ Items in base_path:", os.listdir(base_path))


def find_dir_case_insensitive(parent, name_lower):
    for d in os.listdir(parent):
        if d.lower() == name_lower and os.path.isdir(os.path.join(parent,d)):
            return os.path.join(parent, d), deturn None, None


month_order = MONTHS_LOWER[:]
frames = []
files_read = 0
skipped = 0

for m in month_order:
    mpath, mname_original = find_dir_case_insensitive(base_path, m)
    if mpath is None:
        continue


    for lane_folder in sorted(os.listdir(mpath)):
        lane_path = os.path.join(mpath, lane_folder)
        if not os.path.isdir(lane_path):
            continue

        for fname in os.listdir(lane_path):
            if fname.startswith("~$"):
                continue
            fpath = os.path.join(lane_path, fname)
            try:
                if fname.lower().endswith((".xlsx", ".xls")):
                    df = pd.read_excel(fpath)
                elif fname.lower().endswith(".csv"):
                    df = pd.read_csv(fpath)
                else:
                    skipped += 1
                    continue
            except Exception as e:
                print(f"‚ö†Ô∏è Skipped (read error): {fpath} -> {e}")
                skipped += 1
                continue





original_cols = list(ca_raw.columns)
lower_cols = [c.lower() for c in original_cols]
ca_raw.columns = lower_cols


CANDIDATES = {
    "year":       ["year","yr"],
    "month_num":  ["month","mon"],
    "day":        ["day","date","day_of_month","d"],
    "hour":       ["hour","hr","time_hour"],
    "lane":       ["lane","lane_id","ln"],
    "class":      ["class","vehicle_class","veh_class"],
    "speed":      ["speed","spd","veh_speed"],
    "num_axles":  ["num_axles","axles","numaxles","no_of_axles","n_axles"],
    "gvw":        ["gvw","gross_vehicle_weight","gross","weight","total_weight","gvw_lbs","gvw_kg"],

}

def pick_col(df, options):
    for name in options:
        if name in df.columns:
            return name
    return None


col_year      = pick_col(ca_raw, CANDIDATES["year"])
col_monthnum  = pick_col(ca_raw, CANDIDATES["month_num"])
col_day       = pick_col(ca_raw, CANDIDATES["day"])
col_hour      = pick_col(ca_raw, CANDIDATES["hour"])
col_lane      = pick_col(ca_raw, CANDIDATES["lane"])
col_class     = pick_col(ca_raw, CANDIDATES["class"])
col_speed     = pick_col(ca_raw, CANDIDATES["speed"])
col_numax     = pick_col(ca_raw, CANDIDATES["num_axles"])
col_gvw       = pick_col(ca_raw, CANDIDATES["gvw"])


if col_gvw is None:
    raise KeyError(f"Couldn't find a GVW column in {list(ca_raw.columns)}")


need_cols = [c for c in [col_year, col_monthnum, "month_text", col_day, col_hour,
                         col_lane, col_class, col_speed, col_numax, col_gvw] if c is not None]
ca = ca_raw[need_cols].copy()


rename_map = {}
if col_year:     rename_map[col_year]     = "year"
if col_monthnum: rename_map[col_monthnum] = "month"
rename_map["month_text"]                  = "Month"
if col_day:      rename_map[col_day]      = "day"
if col_hour:     rename_map[col_hour]     = "hour"
if col_lane:     rename_map[col_lane]     = "lane"
if col_class:    rename_map[col_class]    = "class"
if col_speed:    rename_map[col_speed]    = "speed"
if col_numax:    rename_map[col_numax]    = "num_axles"
rename_map[col_gvw]                       = "gvw"

ca = ca.rename(columns=rename_map)


if "month" not in ca.columns:
    month_map = {m:i+1 for i,m in enumerate(MONTHS_LOWER)}

for col in ['year','day','hour','lane','class','speed','num_axles']:
    if col not in ca.columns:
        ca[col] = np.nan


ca["Month"] = ca["Month"].astype(str).str.lower()
ca["month"] = pd.to_numeric(ca["month"], errors="coerce")

if not np.issubdtype(ca["lane"].dtype, np.number):
    ca["lane"] = ca["lane"].astype(str).str.extract(r'(\d+)')
ca["lane"] = pd.to_numeric(ca["lane"], errors="coerce")

for c in ["year","day","hour","class","speed","num_axles","gvw"]:
    ca[c] = pd.to_numeric(ca[c], errors="coerce")


ca = ca.dropna(subset=["gvw"]).reset_index(drop=True)
ca = ca.sort_values(by=["year","month","lane"], kind="mergesort").reset_index(drop=True)


final_order = ['year','month','Month','day','hour','lane','class','speed','num_axles','gvw']

for col in final_order:
    if col not in ca.columns:
        ca[col] = np.nan
ca = ca[final_order]

print("‚úÖ Final California master shape:", ca.shape)
print(ca.head(3))

out_csv = "/content/California_Combined_Ordered.csv"
ca.to_csv(out_csv, index=False)
print("üíæ Saved:", out_csv)

from google.colab import files
files.download(out_csv)

import pandas as pd


ca = pd.read_csv("/content/California_Combined_Ordered.csv")

print("‚úÖ Loaded California Master File:", ca.shape)
print(ca.head())

import seaborn as sns
import matplotlib.pyplot as plt

def analyze_lane(ca_df, lane_number, out_prefix="California"):
    lane_df = ca_df[ca_df["lane"] == lane_number].copy()


    cols_focus = ["hour","class","speed","num_axles","gvw"]
    lane_focus = lane_df[cols_focus].dropna()

    corr = lane_focus.corr()
    out_csv = f"/content/{out_prefix}_Correlation_lane{lane_number}.csv"
    corr.to_csv(out_csv)
    print(f"üíæ Saved correlation matrix for Lane {lane_number} ‚Üí {out_csv}")


    plt.figure(figsize=(6,5))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Correlation Heatmap - Lane {lane_number}")
    plt.tight_layout()
    plt.savefig(f"/content/{out_prefix}_Heatmap_lane{lane_number}.png", dpi=300)
    plt.show()


    for feat in ["hour","class","speed","num_axles"]:
        plt.figure(figsize=(6,4))
        sns.scatterplot(x=feat, y="gvw", data=lane_focus, alpha=0.3)
        plt.title(f"Lane {lane_number} - {feat} vs GVW")
        plt.tight_layout()
        plt.savefig(f"/content/{out_prefix}_Scatter_{feat}_lane{lane_number}.png", dpi=300)
        plt.show()


    plt.figure(figsize=(12,6))
    sns.boxplot(x="hour", y="gvw", data=lane_focus, showfliers=False)
    plt.title(f"Lane {lane_number} - GVW Distribution by Hour")
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.savefig(f"/content/{out_prefix}_Boxplot_GVW_byHour_lane{lane_number}.png", dpi=300)
    plt.show()

print(ca["lane"].unique())
print(ca["lane"].value_counts().head(10))

import pandas as pd

ca = pd.read_csv("/content/California_Combined_Ordered.csv")
print("Loaded:", ca.shape)
print("Lanes present:", ca["lane"].unique())

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os


out_dir = "/content/California_results_lanes"
os.makedirs(out_dir, exist_ok=True)

def analyze_lane_california(df, lane_number):
    print(f"\n=== Lane {lane_number} ===")

    cols_focus = ["hour","class","speed","num_axles","gvw"]
    df_lane = df[df["lane"] == lane_number][cols_focus].copy()

    for c in cols_focus:
        df_lane[c] = pd.to_numeric(df_lane[c], errors="coerce")
    df_lane = df_lane.dropna(subset=["gvw"])
    print("Rows in lane after clean:", len(df_lane))

    if len(df_lane) == 0:
        print("‚ö†Ô∏è No data after cleaning; skipping plots.")
        return


    corr = df_lane.corr(numeric_only=True)
    corr_path = f"{out_dir}/California_Correlation_lane{lane_number}.csv"
    corr.to_csv(corr_path)
    print("Saved:", corr_path)


    plt.figure(figsize=(6,5))
    sns.heatmap(corr, annot=True, cmap="RdBu_r", center=0, fmt=".2f")
    plt.title(f"Correlation Heatmap - Lane {lane_number}")
    heatmap_path = f"{out_dir}/California_Heatmap_lane{lane_number}.png"
    plt.tight_layout()
    plt.savefig(heatmap_path, dpi=300)
    plt.close()
    print("Saved:", heatmap_path)


    n = len(df_lane)
    sample_size = 50000 if n > 50000 else n
    df_samp = df_lane.sample(n=sample_size, random_state=42) if sample_size < n else df_lane
    print(f"Scatter sample size: {len(df_samp)} of {n}")

    for feat in ["hour","class","speed","num_axles"]:
        plt.figure(figsize=(6,4))
        sns.scatterplot(x=df_samp[feat], y=df_samp["gvw"], alpha=0.15, s=8)
        plt.title(f"Lane {lane_number} - {feat} vs GVW (sampled)")
        plt.xlabel(feat); plt.ylabel("GVW")
        scat_path = f"{out_dir}/California_Scatter_{feat}_lane{lane_number}.png"
        plt.tight_layout()
        plt.savefig(scat_path, dpi=300)
        plt.close()
        print("Saved:", scat_path)


    y_cap = float(df_lane["gvw"].quantile(0.995))

    y_min = 0.0

    plt.figure(figsize=(12,6))
    sns.boxplot(x="hour", y="gvw", data=df_lane, showfliers=False)
    plt.ylim(y_min, y_cap)
    plt.title(f"Lane {lane_number} - GVW Distribution by Hour")
    plt.xlabel("Hour of Day"); plt.ylabel("GVW")
    plt.xticks(rotation=90)
    box_path = f"{out_dir}/California_Boxplot_GVW_byHour_lane{lane_number}.png"
    plt.tight_layout()
    plt.savefig(box_path, dpi=300)
    plt.close()
    print("Saved:", box_path)

lanes = sorted(ca["lane"].dropna().unique())
for ln in lanes:
    analyze_lane_california(ca, int(ln))

import shutil
zip_path = "/content/California_results_lanes.zip"
shutil.make_archive(zip_path.replace(".zip",""), 'zip', out_dir)

from google.colab import files
files.download(zip_path)

import pandas as pd

ca = pd.read_csv("/content/California_Combined_Ordered.csv")

audit = (ca
         .assign(has_num_axles = ca["num_axles"].notna())
         .groupby("lane")
         .agg(total_rows=("num_axles","size"),
              non_null=("has_num_axles","sum"))
         .assign(pct_non_null=lambda d: 100*d["non_null"]/d["total_rows"]))

print(audit)
print("\nUnique num_axles values per lane (first 10 each):")
for ln, sub in ca.groupby("lane"):
    print(f"Lane {ln}:", sorted(sub["num_axles"].dropna().unique())[:10])

import os, zipfile, pandas as pd



root = "/content/California"


def list_columns_in_some_files(root, limit=10):
    shown = 0
    for dirpath, dirnames, filenames in os.walk(root):
        for fn in filenames:
            if fn.lower().endswith((".xlsx",".xls",".csv")):
                fpath = os.path.join(dirpath, fn)
                try:
                    if fn.lower().endswith(".csv"):
                        df = pd.read_csv(fpath, nrows=3)
                    else:
                        df = pd.read_excel(fpath, nrows=3)
                    cols = [c.strip() for c in df.columns.astype(str)]
                    axle_like = [c for c in cols if "axle" in c.lower()]
                    print("üìÑ", fpath)
                    print("   columns:", cols)
                    print("   axle-like:", axle_like)
                    print("-"*80)
                    shown += 1
                    if shown >= limit:
                        return
                except Exception as e:
                    print("‚ö†Ô∏è Could not read:", fpath, "->", e)

list_columns_in_some_files(root, limit=10)

import os, sys, numpy as np, pandas as pd

base_path = "/content/California"
MONTHS_LOWER = ['january','february','march','april','may','june',
                'july','august','september','october','november','december']

def has_months(folder):
    try:
        names = [d.lower() for d in os.listdir(folder)
                 if os.path.isdir(os.path.join(folder, d))]
    except Exception:
        return False
    return any(m in names for m in MONTHS_LOWER)

if has_months(base_path):
    months_root = base_path
else:
    months_root = None
    for d in os.listdir(base_path):
        p = os.path.join(base_path, d)
        if os.path.isdir(p) and has_months(p):
            months_root = p
            break
if not months_root:
    print("‚ùå Could not find month folders under", base_path)
    sys.exit(1)

def find_dir_case_insensitive(parent, name_lower):
    for d in os.listdir(parent):
        if d.lower() == name_lower and os.path.isdir(os.path.join(parent, d)):
            return os.path.join(parent, d), d
    return None, None


ALIASES = {
    "year":       ["year","yr"],
    "month":      ["month","mon"],
    "day":        ["day","date","day_of_month","d"],
    "hour":       ["hour","hr","time_hour","hour_of_day"],
    "lane":       ["lane","lane_id","ln"],
    "class":      ["class","vehicle_class","veh_class","vehclass","fhwa_class"],
    "speed":      ["speed","spd","veh_speed"],

    "num_axles":  ["num_axles","axles","axle_count","no_of_axles","n_axles","numaxles","axle no.","axle#","axlecount"],
    "gvw":        ["gvw","gross_vehicle_weight","gross","weight","total_weight","gvw_lbs","gvw_kg"]
}

def pick(df_cols_lower, options_lower):
    for name in options_lower:
        if name in df_cols_lower:
            return name
    return None

def standardize_columns(df):

    df = df.copy()
    orig_cols = list(df.columns)
    lower_cols = [str(c).strip().lower() for c in orig_cols]
    df.columns = lower_cols


    chosen = {}
    for canon, opts in ALIASES.items():
        hit = pick(lower_cols, [o.lower() for o in opts])
        if hit is not None:
            chosen[canon] = hit

    out = pd.DataFrame()
    if "year" in chosen:       out["year"] = df[chosen["year"]]
    if "month" in chosen:      out["month"] = df[chosen["month"]]
    if "day" in chosen:        out["day"] = df[chosen["day"]]
    if "hour" in chosen:       out["hour"] = df[chosen["hour"]]
    if "lane" in chosen:       out["lane"] = df[chosen["lane"]]
    if "class" in chosen:      out["class"] = df[chosen["class"]]
    if "speed" in chosen:      out["speed"] = df[chosen["speed"]]
    if "num_axles" in chosen:  out["num_axles"] = df[chosen["num_axles"]]
    if "gvw" in chosen:        out["gvw"] = df[chosen["gvw"]]

    return out

frames = []
files_read = 0
skipped = 0

for m in MONTHS_LOWER:
    mpath, _ = find_dir_case_insensitive(months_root, m)
    if mpath is None:
        continue

    for lane_folder in sorted(os.listdir(mpath)):
        lane_path = os.path.join(mpath, lane_folder)
        if not os.path.isdir(lane_path):
            continue

        for fn in os.listdir(lane_path):
            if fn.startswith("~$"):                continue
            fpath = os.path.join(lane_path, fn)
            try:
                if fn.lower().endswith(".csv"):
                    df = pd.read_csv(fpath)
                elif fn.lower().endswith((".xlsx",".xls")):
                    df = pd.read_excel(fpath)
                else:
                    skipped += 1
                    continue
            except Exception as e:
                print("‚ö†Ô∏è Read error:", fpath, "->", e)
                skipped += 1
                continue

            std = standardize_columns(df)
            if std.empty:
                skipped += 1
                continue

            std["Month"] = m
            std["lane_folder"] = lane_folder
            frames.append(std)
            files_read += 1

print(f"üì¶ Files read: {files_read}, skipped: {skipped}")
ca = pd.concat(frames, ignore_index=True)
print("Raw standardized shape:", ca.shape)


ca["Month"] = ca["Month"].astype(str).str.lower()


for c in ["year","month","day","hour","class","speed","num_axles","gvw"]:
    if c in ca.columns:
        ca[c] = pd.to_numeric(ca[c], errors="coerce")


if ca["month"].isna().mean() > 0.5:
    m2num = {m:i+1 for i,m in enumerate(MONTHS_LOWER)}
    ca["month"] = ca["Month"].map(m2num)


if "lane" in ca.columns and ca["lane"].notna().any():
    pass
else:
    ca["lane"] = ca["lane_folder"].astype(str).str.extract(r'(\d+)')

ca["lane"] = pd.to_numeric(ca["lane"], errors="coerce")


ca = ca.dropna(subset=["gvw"]).reset_index(drop=True)
ca = ca.sort_values(by=["year","month","lane"], kind="mergesort").reset_index(drop=True)


final_cols = ['year','month','Month','day','hour','lane','class','speed','num_axles','gvw']
for col in final_cols:
    if col not in ca.columns:
        ca[col] = np.nan
ca = ca[final_cols]

print("‚úÖ Final California shape:", ca.shape)
print("Non-null % num_axles:", 100*ca["num_axles"].notna().mean())


out_csv = "/content/California_Combined_Ordered.csv"
ca.to_csv(out_csv, index=False)
print("üíæ Saved:", out_csv)

check = (ca
         .assign(has_ax = ca["num_axles"].notna())
         .groupby("lane")
         .agg(total=("num_axles","size"), non_null=("has_ax","sum"))
         .assign(pct=lambda d: 100*d["non_null"]/d["total"]))
print(check)

import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
import shutil


master_path = "/content/California_Combined_Ordered.csv"
ca = pd.read_csv(master_path)
print("Loaded:", ca.shape)


ca["lane"] = ca["lane"].astype(str).str.extract(r"(\d+)").astype(float)
ca["lane"] = ca["lane"].astype("Int64")
lanes = sorted([int(x) for x in ca["lane"].dropna().unique()])
print("Lanes detected:", lanes)


out_dir = "/content/California_corr_results"
os.makedirs(out_dir, exist_ok=True)


def run_lane_corr(df, lane_num, out_dir, prefix="California"):
    print(f"\n=== Lane {lane_num} ===")
    focus = ["hour","class","speed","num_axles","gvw"]


    lane_df = df[df["lane"] == lane_num].copy()
    existing = [c for c in focus if c in lane_df.columns]
    lane_df = lane_df[existing]


    for c in lane_df.columns:
        lane_df[c] = pd.to_numeric(lane_df[c], errors="coerce")


    if "gvw" not in lane_df.columns:
        print("‚ö†Ô∏è No 'gvw' column present; skipping this lane.")
        return
    lane_df = lane_df.dropna(subset=["gvw"])
    print("Rows (after GVW dropna):", len(lane_df))

    if len(lane_df) == 0:
        print("‚ö†Ô∏è Lane is empty after cleaning; skipping.")
        return


    if "num_axles" in lane_df.columns and lane_df["num_axles"].notna().sum() == 0:
        print("‚ÑπÔ∏è 'num_axles' is empty for this lane ‚Äî will exclude from correlation.")
        lane_df = lane_df.drop(columns=["num_axles"])


    keep_cols = [c for c in lane_df.columns if lane_df[c].std(skipna=True) > 0]
    if "gvw" not in keep_cols:
        print("‚ö†Ô∏è 'gvw' has zero variance; skipping lane.")
        return
    lane_df = lane_df[keep_cols]


    corr = lane_df.corr(numeric_only=True, method="pearson")


    corr_csv = os.path.join(out_dir, f"{prefix}_Correlation_lane{lane_num}.csv")
    corr.to_csv(corr_csv)
    print("üíæ Saved matrix:", corr_csv)


    if "gvw" in corr.columns:
        gvws = corr["gvw"].drop(labels=["gvw"], errors="ignore").sort_values(ascending=False)
        gvws_csv = os.path.join(out_dir, f"{prefix}_CorrWithGVW_lane{lane_num}.csv")
        gvws.to_csv(gvws_csv, header=["corr_with_gvw"])
        print("üíæ Saved corr-with-GVW:", gvws_csv)


    plt.figure(figsize=(6,5))
    sns.heatmap(corr, annot=True, cmap="RdBu_r", center=0, fmt=".2f")
    plt.title(f"Correlation Heatmap - Lane {lane_num}")
    plt.tight_layout()
    heatmap_png = os.path.join(out_dir, f"{prefix}_Heatmap_lane{lane_num}.png")
    plt.savefig(heatmap_png, dpi=300)
    plt.close()
    print("üñºÔ∏è Saved heatmap:", heatmap_png)


for ln in lanes:
    run_lane_corr(ca, ln, out_dir, prefix="California")


zip_path = "/content/California_correlations.zip"
shutil.make_archive(zip_path.replace(".zip",""), "zip", out_dir)
print("\nüì¶ Zipped all results to:", zip_path)
files.download(zip_path)

from google.colab import files
uploaded = files.upload()

import zipfile, os

zip_path = "/content/California.zip"
extract_to = "/content/California"

with zipfile.ZipFile(zip_path, "r") as z:
    z.extractall(extract_to)

print("Unzipped to:", extract_to)
print("Top-level:", os.listdir(extract_to))

import pandas as pd

seen = 0
for dp, dn, fns in os.walk(base_path):
    for fn in fns:
        if not fn.lower().endswith((".xlsx", ".xls", ".csv")):
            continue
        fpath = os.path.join(dp, fn)
        try:
            df = pd.read_csv(fpath, nrows=3) if fn.lower().endswith(".csv") else pd.read_excel(fpath, nrows=3)
            cols = [str(c).strip() for c in df.columns]
            axle_like = [c for c in cols if "axle" in c.lower()]
            print("üìÑ", fpath)
            print("   columns:", cols)
            print("   axle-like:", axle_like)
            print("-"*80)
            seen += 1
            if seen >= 6:
                break
        except Exception as e:
            print("‚ö†Ô∏è Could not read:", fpath, "->", e)
    if seen >= 6:
        break

import numpy as np
import pandas as pd
import os, sys

ALIASES = {
    "year":       ["year","yr"],
    "month":      ["month","mon"],
    "day":        ["day","date","day_of_month","d"],
    "hour":       ["hour","hr","time_hour","hour_of_day"],
    "lane":       ["lane","lane_id","ln","laneno","lane no.","lane_number","lane number"],
    "class":      ["class","vehicle_class","veh_class","vehclass","fhwa_class"],
    "speed":      ["speed","spd","veh_speed","speed_mph","speed(km/h)","speed_kmh"],
    le_count","axle count","no_of_axles","no. of axles",
                   "n_axles","numaxles","axle no.","axle_no","axle#","axlecount","axle_number","axle numbers"],
    "gvw":        ["gvw","gross_vehicle_weight","gross","weight","total_weight","gvw_lbs","gvw (lbs)","gvw_kg","gross weight"]
}

def pick(df_cols_lower, options_lower):
    """Pick the first matching column name (lowercase)."""
    for name in options_lower:
        if name in df_cols_lower:
            return name
    return None

def standardize_frame(raw_df):
    """Return a standardized DataFrame with canonical columns where present."""
    df = raw_df.copy()
       lower_cols = [str(c).strip().lower() for c in df.columns]
    df.columns = lower_cols

    chosen = {}
    for canon, opts in ALIASES.items():
        hit = pick(lower_cols, [o.lower() for o in opts])
        if hit is not None:
            chosen[canon] = hit

    out = pd.DataFrame()
    if "year" in chosen:       out["year"] = df[chosen["year"]]
    if "month" in chosen:      out["month"] = df[chosen["month"]]
    if "day" in chosen:        out["day"] = df[chosen["day"]]
    if "hour" in chosen:       out["hour"] = df[chosen["hour"]]
    if "lane" in chosen:       out["lane"] = df[chosen["lane"]]
    if "class" in chosen:      out["class"] = df[chosen["class"]]
    if "speed" in chosen:      out["speed"] = df[chosen["speed"]]
    if "num_axles" in chosen:  out["num_axles"] = df[chosen["num_axles"]]
    if "gvw" in chosen:        out["gvw"] = df[chosen["gvw"]]
    return out, chosen

def find_dir_case_insensitive(parent, name_lower):
    for d in os.listdir(parent):
        if d.lower() == name_lower and os.path.isdir(os.path.join(parent, d)):
            return os.path.join(parent, d), d
    return None, None

frames = []
files_read = 0
skipped = 0
match_log = {"num_axles": {}, "gvw": {}, "lane": {}, "class": {}, "speed": {}, "year": {}, "month": {}, "day": {}, "hour": {}}

for m in MONTHS_LOWER:
    mpath, mname = find_dir_case_insensitive(base_path, m)
    if mpath is None:
        continue
    for lane_folder in sorted(os.listdir(mpath)):
        lpath = os.path.join(mpath, lane_folder)
        if not os.path.isdir(lpath):
            continue
        for fn in os.listdir(lpath):
            if not fn.lower().endswith((".xlsx",".xls",".csv")):
                continue
            if fn.startswith("~$"):
                continue
            fpath = os.path.join(lpath, fn)
            try:
                raw = pd.read_csv(fpath) if fn.lower().endswith(".csv") else pd.read_excel(fpath)
            except Exception as e:
                print("‚ö†Ô∏è Read error:", fpath, "->", e)
                skipped += 1
                continue

            std, chosen = standardize_frame(raw)
            if std.empty or "gvw" not in std.columns:

                skipped += 1
                continue


            for key in match_log.keys():
                if key in chosen:
                    used = chosen[key]
                    match_log[key][used] = match_log[key].get(used, 0) + 1


            std["Month"] = m
            std["lane_folder"] = lane_folder

            frames.append(std)
            files_read += 1

print(f"üì¶ Files read OK: {files_read}, skipped: {skipped}")

import pprint
print("üîé Alias matches used across files:")
pprint.pprint({k: dict(sorted(v.items(), key=lambda x: -x[1])) for k,v in match_log.items()})

if not frames:
    raise RuntimeError("No usable files were read (no GVW found). Check your data/aliases.")

ca = pd.concat(frames, ignore_index=True)
print("Raw standardized shape:", ca.shape)


ca["Month"] = ca["Month"].astype(str).str.lower()

t
for c in ["year","month","day","hour","class","speed","num_axles","gvw"]:
    if c in ca.columns:
        ca[c] = pd.to_numeric(ca[c], errors="coerce")

if "month" not in ca.columns or ca["month"].isna().mean() > 0.5:
    m2num = {m:i+1 for i,m in enumerate(MONTHS_LOWER)}
    ca["month"] = ca["Month"].map(m2num)

if "lane" not in ca.columns or ca["lane"].isna().mean() > 0.5:
    ca["lane"] = ca["lane_folder"].astype(str).str.extract(r"(\d+)")
ca["lane"] = pd.to_numeric(ca["lane"], errors="coerce")

ca = ca.dropna(subset=["gvw"]).reset_index(drop=True)


ca = ca.sort_values(by=["year","month","lane"], kind="mergesort").reset_index(drop=True)


final_cols = ['year','month','Month','day','hour','lane','class','speed','num_axles','gvw']
for col in final_cols:
    if col not in ca.columns:
        ca[col] = np.nan
ca = ca[final_cols]

print("‚úÖ Final California shape:", ca.shape)
print("Non-null % per column:")
print((100*ca.notna().mean()).round(1))

ax_audit = (ca.assign(has_ax = ca["num_axles"].notna())
              .groupby("lane")
              .agg(total=("num_axles","size"), non_null=("has_ax","sum"))
              .assign(pct=lambda d: 100*d["non_null"]/d["total"]))
print(ax_audit)

import os, sys

BASE = "/content/California"

MONTHS = ['january','february','march','april','may','june',
          'july','august','september','october','november','december']

def has_months(folder):
    try:
        return any(d.lower() in MONTHS for d in os.listdir(folder)
                   if os.path.isdir(os.path.join(folder, d)))
    except Exception:
        return False

if not has_months(BASE):

    candidate = None
    for d in os.listdir(BASE):
        p = os.path.join(BASE, d)
        if os.path.isdir(p) and has_months(p):
            candidate = p
            break
    if candidate is None:
        print("‚ùå Could not find month folders under", BASE)
        sys.exit(1)
    BASE = candidate

print("üìÅ Using base:", BASE)
print("üìÇ Months present:", [d for d in os.listdir(BASE) if os.path.isdir(os.path.join(BASE, d))])

import pandas as pd
import numpy as np
import os
import re


ALIASES = {
    "year":   ["year","yr"],
    "month":  ["month","mon"],
    "day":    ["day","date","day_of_month","d"],
    "hour":   ["hour","hr","time_hour","hour_of_day"],
    "lane":   ["lane","lane_id","ln","lane no.","lane_number","lane number","laneno"],
    "class":  ["class","vehicle_class","veh_class","vehclass","fhwa_class"],
    "speed":  ["speed","spd","veh_speed","speed_mph","speed (mph)","speed_kmh","speed (km/h)"],
    "gvw":    ["gvw","gross_vehicle_weight","gross","weight","total_weight","gvw_lbs","gvw (lbs)","gvw_kg","gross weight"],

    "num_axles": ["num_axles","number of axles","axles","axle_count","axle count","no_of_axles",
                  "no. of axles","n_axles","numaxles","axle no.","axle_no","axle#","axlecount"]
}

def pick_first(df_lower_cols, candidates):
    """Return first matching lowercase name from candidates or None."""
    cands = [c.lower() for c in candidates]
    for c in cands:
        if c in df_lower_cols:
            return c
    return None

def find_dir_case_insensitive(parent, name_lower):
    for d in os.listdir(parent):
        if d.lower() == name_lower and os.path.isdir(os.path.join(parent, d)):
            return os.path.join(parent, d)
    return None

def standardize_one(raw_df, month_text, lane_folder):
    df = raw_df.copy()
    df.columns = [str(c).strip().lower() for c in df.columns]
    out = {}

    for canon, opts in ALIASES.items():
        hit = pick_first(list(df.columns), opts)
        if hit is not None:
            out[canon] = df[hit]


    if "gvw" not in out:
        return pd.DataFrame()

    out_df = pd.DataFrame(out)
    out_df["Month"] = month_text.lower()
    out_df["lane_folder"] = lane_folder
    return out_df

frames, read_ok, skipped = [], 0, 0

for m in MONTHS:
    mpath = find_dir_case_insensitive(BASE, m)
    if not mpath:
        continue

    for lane_folder in sorted(os.listdir(mpath)):
        lpath = os.path.join(mpath, lane_folder)
        if not os.path.isdir(lpath):
            continue

        for fn in os.listdir(lpath):
            if not fn.lower().endswith((".xlsx",".xls",".csv")):
                continue
            if fn.startswith("~$"):
                continue
            fpath = os.path.join(lpath, fn)
            try:
                raw = pd.read_csv(fpath) if fn.lower().endswith(".csv") else pd.read_excel(fpath)
            except Exception:
                skipped += 1
                continue

            std = standardize_one(raw, month_text=m, lane_folder=lane_folder)
            if std.empty:
                skipped += 1
                continue

            frames.append(std)
            read_ok += 1

print(f"üì¶ Files read OK: {read_ok}, skipped: {skipped}")
if not frames:
    raise RuntimeError("No usable files were read (must contain GVW).")

ca = pd.concat(frames, ignore_index=True)
print("Raw standardized shape:", ca.shape)


for c in ["year","month","day","hour","class","speed","num_axles","gvw"]:
    if c in ca.columns:
        ca[c] = pd.to_numeric(ca[c], errors="coerce")


if "month" not in ca.columns or ca["month"].isna().mean() > 0.5:
    m2n = {m:i+1 for i,m in enumerate(MONTHS)}
    ca["month"] = ca["Month"].map(m2n)


if "lane" not in ca.columns or ca["lane"].isna().mean() > 0.5:
    ca["lane"] = ca["lane_folder"].astype(str).str.extract(r"(\d+)")
ca["lane"] = pd.to_numeric(ca["lane"], errors="coerce")


ca = ca.dropna(subset=["gvw"]).reset_index(drop=True)

ca = ca.sort_values(by=["year","month","lane"], kind="mergesort").reset_index(drop=True)

final_cols = ['year','month','Month','day','hour','lane','class','speed','num_axles','gvw']
for col in final_cols:
    if col not in ca.columns:
        ca[col] = np.nan
ca = ca[final_cols]

ax_audit = (ca.assign(has_ax = ca["num_axles"].notna())
              .groupby("lane")
              .agg(total=("num_axles","size"), non_null=("has_ax","sum"))
              .assign(pct=lambda d: 100*d["non_null"]/d["total"]))
print("\nAxle availability per lane:\n", ax_audit)

out_csv = "/content/California_Combined_Ordered.csv"
ca.to_csv(out_csv, index=False)
print("üíæ Saved:", out_csv)

from google.colab import files
files.download(out_csv)

import os, shutil
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files

ca = pd.read_csv(MASTER)
print("Loaded:", ca.shape)


ca["lane"] = ca["lane"].astype(str).str.extract(r"(\d+)").astype(float)
ca["lane"] = ca["lane"].astype("Int64")
lanes = sorted([int(x) for x in ca["lane"].dropna().unique()])
print("Lanes detected:", lanes)


focus = ["hour","class","speed","num_axles","gvw"]


out_dir = "/content/California_corr_focused"
os.makedirs(out_dir, exist_ok=True)

def lane_corr_and_plots(df, lane_num, prefix="California", sample_scatter=50000, boxplot_cap_quantile=0.995):
    print(f"\n=== Lane {lane_num} ===")

    lane_df = df[df["lane"] == lane_num].copy()
    existing = [c for c in focus if c in lane_df.columns]
    lane_df = lane_df[existing]


    for c in lane_df.columns:
        lane_df[c] = pd.to_numeric(lane_df[c], errors="coerce")
    lane_df = lane_df.dropna(subset=["gvw"])
    n = len(lane_df)
    print("Rows (after requiring GVW):", n)
    if n == 0:
        print("‚ö†Ô∏è No rows for this lane after cleaning; skipping.")
        return


    if "num_axles" in lane_df.columns and lane_df["num_axles"].notna().sum() == 0:
        lane_df = lane_df.drop(columns=["num_axles"])
        print("‚ÑπÔ∏è 'num_axles' empty in this lane; excluded from corr & scatter.")


    keep_cols = [c for c in lane_df.columns if lane_df[c].std(skipna=True) > 0]
    if "gvw" not in keep_cols or len(keep_cols) < 2:
        print("‚ö†Ô∏è Not enough variation to compute correlations; skipping.")
        return
    lane_df = lane_df[keep_cols]


    corr = lane_df.corr(numeric_only=True, method="pearson")

    csv_corr = os.path.join(out_dir, f"{prefix}_Correlation_lane_{lane_num}_focused.csv")
    corr.to_csv(csv_corr)
    print("Saved matrix:", csv_corr)

    if "gvw" in corr.columns:
        gvws = corr["gvw"].drop(labels=["gvw"], errors="ignore").sort_values(ascending=False)
        csv_gvw = os.path.join(out_dir, f"{prefix}_CorrWithGVW_lane_{lane_num}_focused.csv")
        gvws.to_csv(csv_gvw, header=["corr_with_gvw"])
        print("Saved corr-with-GVW:", csv_gvw)


    plt.figure(figsize=(6,5))
    sns.heatmap(corr, annot=True, cmap="RdBu_r", center=0, fmt=".2f")
    plt.title(f"{prefix} Focused Correlation - Lane {lane_num}")
    plt.tight_layout()
    png_corr = os.path.join(out_dir, f"{prefix}_Correlation_lane_{lane_num}_focused.png")
    plt.savefig(png_corr, dpi=300)
    plt.close()
    print("Saved heatmap:", png_corr)


    if n > sample_scatter:
        lane_samp = lane_df.sample(n=sample_scatter, random_state=42)
    else:
        lane_samp = lane_df

    for feat in ["hour","class","speed"] + (["num_axles"] if "num_axles" in lane_df.columns else []):
        plt.figure(figsize=(6,4))
        sns.scatterplot(x=lane_samp[feat], y=lane_samp["gvw"], alpha=0.15, s=8)
        plt.title(f"{prefix} Scatter: {feat} vs GVW - Lane {lane_num}")
        plt.xlabel(feat); plt.ylabel("GVW")
        scat_path = os.path.join(out_dir, f"{prefix}_Scatter_{feat}_lane_{lane_num}.png")
        plt.tight_layout(); plt.savefig(scat_path, dpi=300); plt.close()
        print("Saved scatter:", scat_path)


    if "hour" in lane_df.columns:
        y_cap = float(lane_df["gvw"].quantile(boxplot_cap_quantile))
        plt.figure(figsize=(12,6))
        sns.boxplot(x="hour"
        plt.title(f"{prefix} Boxplot: GVW by Hour - Lane {lane_num}")
        plt.xlabel("Hour"); plt.ylabel("GVW")
        plt.xticks(rotation=90)
        box_path = os.path.join(out_dir, f"{prefix}_Boxplot_GVW_byHour_lane_{lane_num}.png")
        plt.tight_layout(); plt.savefig(box_path, dpi=300); plt.close()
        print("Saved boxplot:", box_path)
    else:
        print("‚ÑπÔ∏è No 'hour' column for this lane; skipping boxplot.")


for ln in lanes:
    lane_corr_and_plots(ca, ln)

zip_path = "/content/California_corr_focused.zip"
shutil.make_archive(zip_path.replace(".zip",""), "zip", out_dir)
print("\nZipped:", zip_path)
files.download(zip_path)

import os, shutil
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files

MASTER = "/content/California_Combined_Ordered.csv"
OUT_DIR = "/content/California_heatmaps_allvars"
os.makedirs(OUT_DIR, exist_ok=True)


df = pd.read_csv(MASTER)
df["lane"] = df["lane"].astype(str).str.extract(r"(\d+)").astype(float)
df["lane"] = df["lane"].astype("Int64")
lanes = sorted([int(x) for x in df["lane"].dropna().unique()])
print("Lanes:", lanes)


cols = ["hour", "class", "speed", "num_axles", "gvw"]


for c in cols:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    else:

        df[c] = np.nan


for ln in lanes:
    lane_df = df[df["lane"] == ln][cols].copy()

    lane_df = lane_df.dropna(subset=["gvw"])
    print(f"Lane {ln} rows (GVW present):", len(lane_df))


    corr_p = lane_df.corr(method="pearson")
    csv_p = os.path.join(OUT_DIR, f"California_Correlation_lane_{ln}_ALL.csv")
    corr_p.to_csv(csv_p)

    plt.figure(figsize=(6,5))
    sns.heatmap(
        corr_p.reindex(index=cols, columns=cols),
        annot=True, cmap="RdBu_r", center=0, vmin=-1, vmax=1, fmt=".2f", cbar=True,
        square=True
    )
    plt.title(f"California Pearson Correlation (All Vars) - Lane {ln}")
    plt.tight_layout()
    png_p = os.path.join(OUT_DIR, f"California_Heatmap_lane_{ln}_ALL_Pearson.png")
    plt.savefig(png_p, dpi=300)
    plt.close()

    corr_s = lane_df.corr(method="spearman")
    csv_s = os.path.join(OUT_DIR, f"California_Correlation_lane_{ln}_ALL_Spearman.csv")
    corr_s.to_csv(csv_s)

    plt.figure(figsize=(6,5))
    sns.heatmap(
        corr_s.reindex(index=cols, columns=cols),
        annot=True, cmap="RdBu_r", center=0, vmin=-1, vmax=1, fmt=".2f", cbar=True,
        square=True
    )
    plt.title(f"California Spearman Correlation (All Vars) - Lane {ln}")
    plt.tight_layout()
    png_s = os.path.join(OUT_DIR, f"California_Heatmap_lane_{ln}_ALL_Spearman.png")
    plt.savefig(png_s, dpi=300)
    plt.close()

print("\nSaved to:", OUT_DIR)


zip_path = "/content/California_heatmaps_allvars.zip"
shutil.make_archive(zip_path.replace(".zip",""), "zip", OUT_DIR)
files.download(zip_path)

from google.colab import files
uploaded = files.upload()

import zipfile, os

zip_path = "/content/Texas.zip"

with zipfile.ZipFile(zip_path, "r") as z:
    z.extractall(extract_to)

print(" Unzipped to:", extract_to)
print("Top-level:", os.listdir(extract_to))

import os
os.listdir("/content")

import os
print(os.listdir("/content/Texas"))

import pandas as pd
import os

extract_path = "/content/Texas/Texas"

all_dfs = []
for file in os.listdir(extract_path):
    if file.lower().endswith(".csv"):
        file_path = os.path.join(extract_path, file)
        df = pd.read_csv(file_path)


        df.insert(
            df.columns.get_loc("month") + 1,
            "month_name",
            pd.to_datetime(df["month"], format='%m').dt.strftime('%B')
        )

        all_dfs.append(df)


if all_dfs:
    texas_master = pd.concat(all_dfs, ignore_index=True)
    texas_master.to_csv("/content/texas_master.csv", index=False)
    print("‚úÖ Texas master file created at /content/texas_master.csv")
    display(texas_master.head())
else:
    print("‚ö†Ô∏è No CSV files found in the folder")

import pandas as pd
import os


base_path = "/content/Texas"

all_dfs = []
for root, dirs, files in os.walk(base_path):
    for file in files:
        if file.lower().endswith(".csv"):
            file_path = os.path.join(root, file)
            df = pd.read_csv(file_path)


            df.insert(
                df.columns.get_loc("month") + 1,
                "month_name",
                pd.to_datetime(df["month"], format='%m').dt.strftime('%B')
            )

            all_dfs.append(df)


if all_dfs:
    texas_master = pd.concat(all_dfs, ignore_index=True)
    output_path = "/content/texas_master.csv"
    texas_master.to_csv(output_path, index=False)
    print(f"‚úÖ Texas master file created at: {output_path}")
    display(texas_master.head())
else:
    print("‚ö†Ô∏è No CSV files found in Texas folder")

import pandas as pd
import calendar


texas_master = pd.read_csv("/content/texas_master.csv")


month_order = list(calendar.month_name)[1:]


texas_master["month_name"] = pd.Categorical(
    texas_master["month_name"],
    categories=month_order,
    ordered=True
)

texas_master = texas_master.sort_values(by=["year", "month"]).reset_index(drop=True)

output_path = "/content/texas_master.csv"
texas_master.to_csv(output_path, index=False)

print("‚úÖ Texas master file saved with months arranged Jan ‚Üí Dec")
display(texas_master.head(12))

from google.colab import files
files.download("/content/texas_master.csv")

import pandas as pd

texas_master = pd.read_csv("/content/texas_master.csv")

print("Number of rows:", len(texas_master))
print("Number of columns:", len(texas_master.columns))
print("\nMonths present:", texas_master["month_name"].unique())

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


texas_master = pd.read_csv("/content/texas_master.csv")


texas_master.columns = texas_master.columns.str.strip().str.lower()


target = "gvw"
features = ["number of axle", "vehicle class", "hour"]


for lane in sorted(texas_master["lane"].unique()):
    lane_data = texas_master[texas_master["lane"] == lane]

    print(f"\n===============================")
    print(f"         Lane {lane}")
    print(f"===============================")
    print(f"Rows: {len(lane_data)}")

    pearson_corr = lane_data[features + [target]].corr(method="pearson")
    spearman_corr = lane_data[features + [target]].corr(method="spearman")
    kendall_corr = lane_data[features + [target]].corr(method="kendall")

    print("\nPearson Correlation:")
    print(pearson_corr[target].drop(target))

    print("\nSpearman Correlation:")
    print(spearman_corr[target].drop(target))

    print("\nKendall Correlation:")
    print(kendall_corr[target].drop(target))


    plt.figure(figsize=(6,4))
    sns.heatmap(pearson_corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Pearson Correlation Heatmap - Lane {lane}")
    plt.show()

  -
    for col in features:
        plt.figure(figsize=(6,4))
        sns.scatterplot(
            data=lane_data.sample(min(100000, len(lane_data))),
            x=col, y=target, alpha=0.3
        )
        plt.title(f"Scatterplot: {target} vs {col} (Lane {lane})")
        plt.show()

    for col in features:
        plt.figure(figsize=(8,4))
        sns.boxplot(
            data=lane_data.sample(min(50000, len(lane_data))),
            x=col, y=target
        )
        plt.title(f"Boxplot: {target} by {col} (Lane {lane})")
        plt.xticks(rotation=45)
        plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os, zipfile


texas_master = pd.read_csv("/content/texas_master.csv")
texas_master.columns = texas_master.columns.str.strip().str.lower()


target = "gvw"
features = ["number of axle", "vehicle class", "hour"]
output_dir = "/content/texas_correlation_results"

os.makedirs(output_dir, exist_ok=True)


for lane in sorted(texas_master["lane"].unique()):
    lane_data = texas_master[texas_master["lane"] == lane]

    lane_dir = os.path.join(output_dir, f"lane_{lane}")
    os.makedirs(lane_dir, exist_ok=True)


    pearson_corr = lane_data[features + [target]].corr(method="pearson")
    spearman_corr = lane_data[features + [target]].corr(method="spearman")
    kendall_corr = lane_data[features + [target]].corr(method="kendall")


    pearson_corr.to_csv(os.path.join(lane_dir, "pearson_corr.csv"))
    spearman_corr.to_csv(os.path.join(lane_dir, "spearman_corr.csv"))
    kendall_corr.to_csv(os.path.join(lane_dir, "kendall_corr.csv"))


    plt.figure(figsize=(6,4))
    sns.heatmap(pearson_corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"Pearson Heatmap - Lane {lane}")
    plt.tight_layout()
    plt.savefig(os.path.join(lane_dir, "pearson_heatmap.png"))
    plt.close()

        plt.figure(figsize=(6,4))
        sns.scatterplot(
            data=lane_data.sample(min(100000, len(lane_data))),
            x=col, y=target, alpha=0.3
        )
        plt.title(f"Scatter: {target} vs {col} (Lane {lane})")
        plt.tight_layout()
        plt.savefig(os.path.join(lane_dir, f"scatter_{col}.png"))
        plt.close()

    for col in features:
        plt.figure(figsize=(8,4))
        sns.boxplot(
            data=lane_data.sample(min(50000, len(lane_data))),
            x=col, y=target
        )
        plt.title(f"Boxplot: {target} by {col} (Lane {lane})")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(lane_dir, f"boxplot_{col}.png"))
        plt.close()

zip_path = "/content/texas_correlation_results.zip"
with zipfile.ZipFile(zip_path, 'w') as zipf:
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            zipf.write(os.path.join(root, file),
                       os.path.relpath(os.path.join(root, file), output_dir))

print(f"‚úÖ All correlation results saved to: {zip_path}")

from google.colab import files
files.download("/content/texas_correlation_results.zip")

import os
print(os.listdir("/content"))

from google.colab import files
uploaded = files.upload()

import os
print(os.listdir("/content"))

import pandas as pd

ny_master = pd.read_csv("/content/NewYork_Combined_Ordered.csv")
print("Shape:", ny_master.shape)
print("Columns:", ny_master.columns.tolist())
ny_master.head()

print(ny_master.columns.tolist())

import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

ny_master = pd.read_csv("/content/NewYork_Combined_Ordered.csv")
ny_master.columns = ny_master.columns.str.strip().str.lower()


feature = "class"
target = "gvw"


split_ratios = [0.9, 0.8, 0.7, 0.6, 0.5]
params = {
    "max_depth": 6,
    "learning_rate": 0.1,
    "n_estimators": 500,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "reg_lambda": 1,
    "reg_alpha": 0,
    "min_child_weight": 1,
    "objective": "reg:squarederror",
    "random_state": 42
}

results = []


for lane in sorted(ny_master["lane"].unique()):
    lane_data = ny_master[ny_master["lane"] == lane]

    X = lane_data[[feature]]
    y = lane_data[target]

    for ratio in split_ratios:
        train_size = int(len(X) * ratio)

        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
        y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]


        model = XGBRegressor(**params)
        model.fit(X_train, y_train)


        y_pred = model.predict(X_test)


        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

        results.append({
            "Lane": lane,
            "Split": f"{int(ratio*100)}/{int((1-ratio)*100)}",
            "RMSE": round(rmse, 2),
            "MAE": round(mae, 2),
            "MAPE": round(mape, 2)
        })

results_df = pd.DataFrame(results)

results_df.to_csv("/content/NewYork_XGB_Results.csv", index=False)

print("‚úÖ Analysis complete. Results saved to /content/NewYork_XGB_Results.csv")
results_df

from google.colab import files
files.download("/content/NewYork_XGB_Results.csv")

import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error


ny_master = pd.read_csv("/content/NewYork_Combined_Ordered.csv")
ny_master.columns = ny_master.columns.str.strip().str.lower()


feature = "class"
target = "gvw"


params = {
    "max_depth": 6,
    "learning_rate": 0.1,
    "n_estimators": 500,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "reg_lambda": 1,
    "reg_alpha": 0,
    "min_child_weight": 1,
    "objective": "reg:squarederror",
    "random_state": 42
}

results = []


for lane in sorted(ny_master["lane"].unique()):
    lane_data = ny_master[ny_master["lane"] == lane]

    for vclass in range(4, 14):
        class_data = lane_data[lane_data["class"] == vclass]

        if class_data.empty:
            continue

        X = class_data[[feature]]
        y = class_data[target]


        train_size = int(len(X) * 0.7)
        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
        y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]

        if len(X_test) == 0:
            continue


        model = XGBRegressor(**params)
        model.fit(X_train, y_train)


        y_pred = model.predict(X_test)


        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

        results.append({
            "Lane": f"Lane {lane}",
            "Class": vclass,
            "RMSE": round(rmse, 2),
            "MAE": round(mae, 2),
            "MAPE": f"{round(mape, 2)}%"
        })


results_df = pd.DataFrame(results)


results_df.to_csv("/content/NewYork_LaneClass_Results.csv", index=False)

print("‚úÖ Results saved to /content/NewYork_LaneClass_Results.csv")
results_df

from google.colab import files
files.download("/content/NewYork_LaneClass_Results.csv")

import seaborn as sns
import matplotlib.pyplot as plt


results_df = pd.read_csv("/content/NewYork_LaneClass_Results.csv")


results_df["MAPE"] = results_df["MAPE"].astype(str).str.replace('%','').astype(float)


pivot_mape = results_df.pivot(index="Class", columns="Lane", values="MAPE")
plt.figure(figsize=(8,6))
sns.heatmap(pivot_mape, annot=True, cmap="Reds", fmt=".1f")
plt.title("MAPE (%) by Lane and Vehicle Class")
plt.show()

plt.figure(figsize=(10,6))
sns.barplot(data=results_df, x="Class", y="RMSE", hue="Lane")
plt.title("RMSE per Vehicle Class (Lanes 1‚Äì3)")
plt.show()



from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor


ny_master = pd.read_csv("/content/NewYork_Combined_Ordered.csv")
ny_master.columns = ny_master.columns.str.strip().str.lower()


target = "gvw"
feature = "class"

results = []

for lane in sorted(ny_master["lane"].unique()):
    lane_data = ny_master[ny_master["lane"] == lane]

    for cls in sorted(lane_data["class"].unique()):
        class_data = lane_data[lane_data["class"] == cls]

        if len(class_data) < 50:
            continue

        X = class_data[[feature]]
        y = class_data[target]


        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )


        model = XGBRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=3,
            objective="reg:squarederror",
            subsample=1.0,
            colsample_bytree=1.0,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)


        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        mape = (abs((y_test - y_pred) / y_test).mean()) * 100
        r2 = r2_score(y_test, y_pred)

        results.append({
            "Lane": lane,
            "Class": cls,
            "RMSE": round(rmse, 2),
            "MAE": round(mae, 2),
            "MAPE (%)": round(mape, 2),
            "R2": round(r2, 3)
        })


results_df = pd.DataFrame(results)
output_path = "/content/NewYork_XGB_Results.csv"
results_df.to_csv(output_path, index=False)

print(f"‚úÖ Results saved to {output_path}")
print(results_df.head(20))

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

import os
print(os.listdir("/content"))

import zipfile, os

with zipfile.ZipFile("/content/texas_master.zip", 'r') as z:
    z.extractall("/content")

print(os.listdir("/content"))

import pandas as pd

df = pd.read_csv("/content/texas_master.csv", nrows=5)
print(df.columns.tolist())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor

df = pd.read_csv("/content/texas_master.csv")
df.columns = df.columns.str.strip().str.lower()

print("Texas columns:", df.columns.tolist())


target = "gvw"

feature = "vehicle class"

results = []

for lane in sorted(df["lane"].unique()):
    lane_data = df[df["lane"] == lane]

    for cls in sorted(lane_data[feature].unique()):
        class_data = lane_data[lane_data[feature] == cls]

        if len(class_data) < 50:
        X = class_data[[feature]]
        y = class_data[target]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        model = XGBRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=3,
            objective="reg:squarederror",
            subsample=1.0,
            colsample_bytree=1.0,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)


        y_pred = model.predict(X_test)


        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        mape = (abs((y_test - y_pred) / y_test).mean()) * 100

        results.append({
            "Lane": lane,
            "Class": cls,
            "RMSE": round(rmse, 2),
            "MAE": round(mae, 2),
            "MAPE (%)": round(mape, 2)
        })


results_df = pd.DataFrame(results)
output_path = "/content/Texas_XGB_Results.csv"
results_df.to_csv(output_path, index=False)

print(f"‚úÖ Texas results saved to {output_path}")
print(results_df.head(15))

from google.colab import files
files.download("/content/Texas_XGB_Results.csv")

from google.colab import files
files.download("/content/California_XGB_Results.csv")

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor


df = pd.read_csv("/content/NewYork_Combined_Ordered.csv")
df.columns = df.columns.str.strip().str.lower()

print("New York columns:", df.columns.tolist())


target = "gvw"


feature = "class"

results = []


for lane in sorted(df["lane"].unique()):
    lane_data = df[df["lane"] == lane]

    for cls in sorted(lane_data[feature].unique()):
        class_data = lane_data[lane_data[feature] == cls]

        if len(class_data) < 50:
            continue

        X = class_data[[feature]]
        y = class_data[target]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )


        model = XGBRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=3,
            objective="reg:squarederror",
            subsample=1.0,
            colsample_bytree=1.0,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)


        y_pred = model.predict(X_test)


        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        mape = (abs((y_test - y_pred) / y_test).mean()) * 100

        results.append({
            "Lane": lane,
            "Class": cls,
            "RMSE": round(rmse, 2),
            "MAE": round(mae, 2),
            "MAPE (%)": round(mape, 2)
        })

results_df = pd.DataFrame(results)
output_path = "/content/NewYork_XGB_Results.csv"
results_df.to_csv(output_path, index=False)

print(f"‚úÖ New York results saved to {output_path}")
print(results_df.head(15))

from google.colab import files
files.download("/content/NewYork_XGB_Results.csv")

import pandas as pd


ny = pd.read_csv("/content/NewYork_XGB_Results.csv")
ca = pd.read_csv("/content/California_XGB_Results.csv")
tx = pd.read_csv("/content/Texas_XGB_Results.csv")

ny = ny.rename(columns={"RMSE": "NY (RMSE)", "MAE": "NY (MAE)", "MAPE (%)": "NY (MAPE)"})
ca = ca.rename(columns={"RMSE": "CA (RMSE)", "MAE": "CA (MAE)", "MAPE (%)": "CA (MAPE)"})
tx = tx.rename(columns={"RMSE": "TX (RMSE)", "MAE": "TX (MAE)", "MAPE (%)": "TX (MAPE)"})

merged = pd.merge(ny, ca, on=["Lane", "Class"], how="outer")
merged = pd.merge(merged, tx, on=["Lane", "Class"], how="outer")

merged = merged[[
    "Lane", "Class",
    "NY (RMSE)", "NY (MAE)", "NY (MAPE)",
    "CA (RMSE)", "CA (MAE)", "CA (MAPE)",
    "TX (RMSE)", "TX (MAE)", "TX (MAPE)"
]]


output_path = "/content/AllStates_WideResults.csv"
merged.to_csv(output_path, index=False)

print("‚úÖ Wide comparison table saved at:", output_path)
print(merged.head(10))

from google.colab import files
files.download("/content/AllStates_WideResults.csv")

!pip install python-docx

import pandas as pd
from docx import Document
from docx.shared import Pt

ny = pd.read_csv("/content/NewYork_XGB_Results.csv")
ca = pd.read_csv("/content/California_XGB_Results.csv")
tx = pd.read_csv("/content/Texas_XGB_Results.csv")


ny = ny.rename(columns={"RMSE": "NY (RMSE)", "MAE": "NY (MAE)", "MAPE (%)": "NY (MAPE)"})
ca = ca.rename(columns={"RMSE": "CA (RMSE)", "MAE": "CA (MAE)", "MAPE (%)": "CA (MAPE)"})
tx = tx.rename(columns={"RMSE": "TX (RMSE)", "MAE": "TX (MAE)", "MAPE (%)": "TX (MAPE)"})


merged = pd.merge(ny, ca, on=["Lane", "Class"], how="outer")
merged = pd.merge(merged, tx, on=["Lane", "Class"], how="outer")


merged = merged[[
    "Lane", "Class",
    "NY (RMSE)", "NY (MAE)", "NY (MAPE)",
    "CA (RMSE)", "CA (MAE)", "CA (MAPE)",
    "TX (RMSE)", "TX (MAE)", "TX (MAPE)"
]]


excel_path = "/content/AllStates_WideResults.xlsx"
merged.to_excel(excel_path, index=False)


doc = Document()
doc.add_heading("Performance Measures by State", level=1)

table = doc.add_table(rows=1, cols=len(merged.columns))
hdr_cells = table.rows[0].cells
for i, col in enumerate(merged.columns):
    hdr_cells[i].text = col


for row in merged.itertuples(index=False):
    cells = table.add_row().cells
    for i, val in enumerate(row):
        cells[i].text = str(val)


word_path = "/content/AllStates_WideResults.docx"
doc.save(word_path)

print("‚úÖ Results saved as:")
print("Excel:", excel_path)
print("Word:", word_path)

from google.colab import files
files.download("/content/AllStates_WideResults.xlsx")
files.download("/content/AllStates_WideResults.docx")

!pip install python-pptx

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.shapes import MSO_SHAPE


prs = Presentation()


title_slide_layout = prs.slide_layouts[0]
content_slide_layout = prs.slide_layouts[1]
blank_slide_layout = prs.slide_layouts[6]


slide = prs.slides.add_slide(title_slide_layout)
slide.shapes.title.text = "Feature Selection for Vehicle Weight Prediction"
slide.placeholders[1].text = "Using XGBoost and WIM Data (NY, CA, TX)\nYour Name ‚Äì Presentation"


slide = prs.slides.add_slide(content_slide_layout)
slide.shapes.title.text = "Why Feature Selection Matters"
content = slide.placeholders[1].text_frame
content.text = ""
points = [
    "Improve accuracy and computational efficiency",
    "Reduce noise and prevent overfitting",
    "Improve interpretability of the model",
    "Ensure consistency across states‚Äô datasets",
]
for point in points:
    p = content.add_paragraph()
    p.text = "‚Ä¢ " + point
    p.font.size = Pt(20)


slide = prs.slides.add_slide(content_slide_layout)
slide.shapes.title.text = "Feature Selection Process"
content = slide.placeholders[1].text_frame
steps = [
    "Step 1: Exploratory Data Analysis (EDA ‚Äì correlation heatmaps, scatter plots)",
    "Step 2: Identify strong/weak correlations",
    "Step 3: Remove unnecessary/weak variables (e.g., speed not in Texas data)",
    "Step 4: Final features kept: vehicle class, number of axles, hour",
]
for step in steps:
    p = content.add_paragraph()
    p.text = "‚Üí " + step
    p.font.size = Pt(18)

slide = prs.slides.add_slide(blank_slide_layout)
title_shape = slide.shapes.add_textbox(Inches(1), Inches(0.3), Inches(8), Inches(0.5))
title_shape.text = "Workflow for Feature Selection"

flow_steps = [
    ("Data Collection (NY, CA, TX)", RGBColor(91, 155, 213)),
    ("Data Cleaning (filter classes 4‚Äì13, remove GVW=0, align columns)", RGBColor(112, 173, 71)),
    ("Feature Selection (EDA, correlation analysis, expert judgment)", RGBColor(255, 192, 0)),
    ("Final Features Selected: Vehicle Class, Number of Axles, Hour", RGBColor(244, 177, 131)),
    ("Target Variable: GVW", RGBColor(255, 102, 102)),
]

left, top, width, height = Inches(2.5), Inches(1.2), Inches(5), Inches(0.8)
shapes = slide.shapes

prev_shape = None
for step, color in flow_steps:
    shape = shapes.add_shape(MSO_SHAPE.RECTANGLE, left, top, width, height)
    shape.fill.solid()
    shape.fill.fore_color.rgb = color
    shape.text = step
    shape.text_frame.paragraphs[0].font.size = Pt(16)
    shape.text_frame.paragraphs[0].font.color.rgb = RGBColor(255, 255, 255)
    if prev_shape:

        connector = shapes.add_connector(
            1, prev_shape.left + prev_shape.width/2, prev_shape.top + prev_shape.height,
            shape.left + shape.width/2, shape.top
        )
        connector.line.color.rgb = RGBColor(0, 0, 0)
    prev_shape = shape
    top += Inches(1.2)

slide = prs.slides.add_slide(content_slide_layout)
slide.shapes.title.text = "Selected Features vs Target Variable"
rows, cols = 4, 2
table = slide.shapes.add_table(rows, cols, Inches(1), Inches(1.5), Inches(8), Inches(2.5)).table

table.columns[0].width = Inches(3)
table.columns[1].width = Inches(3)


table.cell(0, 0).text = "Independent Variables"
table.cell(0, 1).text = "Target Variable"
for j in range(2):
    table.cell(0, j).text_frame.paragraphs[0].font.bold = True


table.cell(1, 0).text = "Vehicle Class"
table.cell(2, 0).text = "Number of Axles"
table.cell(3, 0).text = "Hour"
table.cell(1, 1).text = "GVW"

slide = prs.slides.add_slide(content_slide_layout)
slide.shapes.title.text = "Visual Evidence (Correlation Heatmap)"
content = slide.placeholders[1].text_frame
content.text = "Heatmaps and scatter plots highlight feature relationships:"
for point in [
    "GVW ‚Üî Vehicle Class ‚Üí Strong positive correlation",
    "GVW ‚Üî Number of Axles ‚Üí Strong positive correlation",
    "GVW ‚Üî Hour ‚Üí Weak correlation, kept for testing",
]:
    p = content.add_paragraph()
    p.text = "‚Ä¢ " + point


output_path = "Feature_Selection_Presentation_Styled.pptx"
prs.save(output_path)
output_path

from google.colab import files
files.download("Feature_Selection_Presentation_Styled.pptx")

!pip install python-pptx

from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.shapes import MSO_SHAPE


prs = Presentation()
title_layout = prs.slide_layouts[0]
content_layout = prs.slide_layouts[1]
blank_layout = prs.slide_layouts[6]


slide = prs.slides.add_slide(title_layout)
slide.shapes.title.text = "Feature Selection for Vehicle Weight Prediction"
slide.placeholders[1].text = "Using XGBoost and WIM Data (NY, CA, TX)\nYour Name ‚Äì Presentation"


slide = prs.slides.add_slide(content_layout)
slide.shapes.title.text = "Why Feature Selection Matters"
content = slide.placeholders[1].text_frame
points = [
    "Improve accuracy and computational efficiency",
    "Reduce noise and prevent overfitting",
    "Improve interpretability of the model",
    "Ensure consistency across datasets (NY, CA, TX)",
]
for p in points:
    para = content.add_paragraph()
    para.text = "‚Ä¢ " + p
    para.font.size = Pt(20)


slide = prs.slides.add_slide(content_layout)
slide.shapes.title.text = "Feature Selection Process"
content = slide.placeholders[1].text_frame
steps = [
    "Step 1: Exploratory Data Analysis (heatmaps, scatter plots)",
    "Step 2: Identify strong/weak correlations",
    "Step 3: Remove weak/unavailable variables (e.g., speed missing in Texas)",
    "Step 4: Final features kept: Vehicle Class, Number of Axles, Hour",
]
for s in steps:
    para = content.add_paragraph()
    para.text = "‚Üí " + s
    para.font.size = Pt(18)


slide = prs.slides.add_slide(blank_layout)
title_shape = slide.shapes.add_textbox(Inches(1), Inches(0.2), Inches(8), Inches(0.6))
title_shape.text = "Workflow for Feature Selection"
title_shape.text_frame.paragraphs[0].font.size = Pt(28)

flow_steps = [
    ("Data Collection (NY, CA, TX)", RGBColor(91, 155, 213)),
    ("Data Cleaning (filter class 4‚Äì13, remove GVW=0, align columns)", RGBColor(112, 173, 71)),
    ("Feature Selection (EDA, correlation analysis, expert judgment)", RGBColor(255, 192, 0)),
    ("Final Features: Vehicle Class, Num. Axles, Hour", RGBColor(244, 177, 131)),
    ("Target Variable: GVW", RGBColor(255, 102, 102)),
]

left, top, width, height = Inches(2.5), Inches(1), Inches(5), Inches(0.9)
prev_shape = None
for step, color in flow_steps:
    box = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, left, top, width, height)
    box.fill.solid()
    box.fill.fore_color.rgb = color
    box.text = step
    box.text_frame.paragraphs[0].font.size = Pt(16)
    box.text_frame.paragraphs[0].font.color.rgb = RGBColor(255, 255, 255)
    if prev_shape:

        connector = slide.shapes.add_connector(
            1,
            prev_shape.left + prev_shape.width/2,
            prev_shape.top + prev_shape.height,
            box.left + box.width/2,
            box.top
        )
        connector.line.color.rgb = RGBColor(0, 0, 0)
    prev_shape = box
    top += Inches(1.3)


slide = prs.slides.add_slide(content_layout)
slide.shapes.title.text = "Selected Features vs Target Variable"
rows, cols = 4, 2
table = slide.shapes.add_table(rows, cols, Inches(1), Inches(1.5), Inches(8), Inches(2.5)).table
table.columns[0].width = Inches(3.5)
table.columns[1].width = Inches(3.5)
table.cell(0, 0).text = "Independent Variables"
table.cell(0, 1).text = "Target Variable"
table.cell(1, 0).text = "Vehicle Class"
table.cell(2, 0).text = "Number of Axles"
table.cell(3, 0).text = "Hour"
table.cell(1, 1).text = "GVW"
for j in range(2):
    table.cell(0, j).text_frame.paragraphs[0].font.bold = True

slide = prs.slides.add_slide(content_layout)
slide.shapes.title.text = "Visual Evidence (Correlation Heatmap)"
content = slide.placeholders[1].text_frame
points = [
    "GVW ‚Üî Vehicle Class ‚Üí Strong positive correlation",
    "GVW ‚Üî Number of Axles ‚Üí Strong positive correlation",
    "GVW ‚Üî Hour ‚Üí Weak correlation, kept for testing",
]
for p in points:
    para = content.add_paragraph()
    para.text = "‚Ä¢ " + p

# Save pptx
output_path = "Feature_Selection_Styled.pptx"
prs.save(output_path)

print("‚úÖ Presentation created:", output_path)

from google.colab import files
files.download("Feature_Selection_Styled.pptx")





import pandas as pd
import io, zipfile
from google.colab import files

print("üìÇ Please upload 'California_Combined_Ordered.zip'")
uploaded = files.upload()


zip_filename = list(uploaded.keys())[0]


with zipfile.ZipFile(io.BytesIO(uploaded[zip_filename]), 'r') as z:

    print("üì¶ Files inside ZIP:")
    z.printdir()


    csv_name = [f for f in z.namelist() if f.lower().endswith('.csv')][0]
    print(f"\n‚úÖ Found CSV file: {csv_name}")


    with z.open(csv_name) as f:
        df_california = pd.read_csv(f, low_memory=False)


print(f"\n‚úÖ California dataset loaded successfully!")
print(f"Shape: {df_california.shape[0]} rows √ó {df_california.shape[1]} columns")
print("\nüîç First five rows:")
display(df_california.head())

print("\nüßæ Column names:")
print(list(df_california.columns))

import pandas as pd
import numpy as np

df_california.columns = df_california.columns.str.strip().str.lower()


cols_to_numeric = ['class', 'num_axles', 'speed', 'gvw']
for col in cols_to_numeric:
    if col in df_california.columns:
        df_california[col] = pd.to_numeric(df_california[col], errors='coerce')


before = len(df_california)
df_california = df_california[
    (df_california['class'] >= 4) & (df_california['class'] <= 13)
]
print(f"üöö Kept vehicle classes 4‚Äì13: {before} ‚Üí {len(df_california)} rows")


before = len(df_california)
df_california = df_california[df_california['gvw'] > 0]
print(f"‚öñÔ∏è Removed invalid GVW (‚â§ 0): {before} ‚Üí {len(df_california)} rows")

before = len(df_california)
df_california = df_california.drop_duplicates()
print(f"üßπ Removed duplicates: {before} ‚Üí {len(df_california)} rows")


missing_summary = df_california[cols_to_numeric].isna().sum()
print("\nüîç Missing-value summary:")
print(missing_summary)


print("\n‚úÖ Cleaned California dataset summary:")
print(df_california.describe())

print("\nüßæ Cleaned columns:")
print(list(df_california.columns))


df_california.to_csv("California_Cleaned.csv", index=False)
print("\nüíæ Saved cleaned dataset as 'California_Cleaned.csv'")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


candidates = [
    'class', 'num_axles', 'speed', 'gvw',

]
use_cols = [c for c in candidates if c in df_california.columns]

if len(use_cols) < 2:

    use_cols = df_california.select_dtypes(include=[np.number]).columns.tolist()

corr_df = df_california[use_cols].copy()


corr_df = corr_df.fillna(corr_df.median(numeric_only=True))

corr = corr_df.corr(method='pearson')


corr.to_csv("CA_correlation_matrix.csv", index=True)
print("üíæ Saved: CA_correlation_matrix.csv")


plt.figure(figsize=(10, 8))
sns.heatmap(
    corr,
    annot=True, fmt=".2f",
    cmap="coolwarm", center=0,
    square=True, linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)
plt.title("Correlation Heatmap ‚Äî California (Aggregated, 2021)", fontsize=14, pad=12)
plt.tight_layout()
plt.savefig("Figure3_CA_Correlation_Heatmap.png", dpi=300, bbox_inches="tight")
plt.show()

print("üñºÔ∏è Saved: Figure3_CA_Correlation_Heatmap.png")

sns.heatmap(
    corr, annot=True, fmt=".2f",
    cmap="RdBu_r", center=0,
    square=True, linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)

import matplotlib.pyplot as plt
import seaborn as sns


plt.figure(figsize=(8, 6))
sns.heatmap(
    corr,
    annot=True, fmt=".2f",
    cmap="RdBu_r", center=0,
    square=True, linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)
plt.title("Correlation Heatmap ‚Äî California (Aggregated, 2021)", fontsize=14, pad=12)
plt.xlabel("Variables", fontsize=12)
plt.ylabel("Variables", fontsize=12)

plt.tight_layout()
plt.savefig("Figure3_CA_Correlation_Heatmap_Final.png", dpi=600, bbox_inches="tight")
plt.savefig("Figure3_CA_Correlation_Heatmap_Final.pdf", bbox_inches="tight")
plt.show()

print("Saved: 'Figure3_CA_Correlation_Heatmap_Final.png' and 'Figure3_CA_Correlation_Heatmap_Final.pdf'")

import pandas as pd
import numpy as np

df_newyork = pd.read_csv("NewYork_Combined_Ordered.csv", low_memory=False)


df_newyork.columns = df_newyork.columns.str.strip().str.lower()

cols_to_numeric = ['class', 'num_axles', 'speed', 'gvw']
for col in cols_to_numeric:
    if col in df_newyork.columns:
        df_newyork[col] = pd.to_numeric(df_newyork[col], errors='coerce')


before = len(df_newyork)
df_newyork = df_newyork[(df_newyork['class'] >= 4) & (df_newyork['class'] <= 13)]
print(f"üöö Kept vehicle classes 4‚Äì13: {before} ‚Üí {len(df_newyork)} rows")


before = len(df_newyork)
df_newyork = df_newyork[df_newyork['gvw'] > 0]
print(f"‚öñÔ∏è Removed invalid GVW (‚â§ 0): {before} ‚Üí {len(df_newyork)} rows")


before = len(df_newyork)
df_newyork = df_newyork.drop_duplicates()
print(f"üßπ Removed duplicates: {before} ‚Üí {len(df_newyork)} rows")


missing_summary = df_newyork[cols_to_numeric].isna().sum()
print("\nüîç Missing-value summary:")
print(missing_summary)


df_newyork.to_csv("NewYork_Cleaned.csv", index=False)
print("\nüíæ Saved cleaned dataset as 'NewYork_Cleaned.csv'")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


cols = ['class', 'num_axles', 'speed', 'gvw']
use_cols = [c for c in cols if c in df_newyork.columns]

corr_df = df_newyork[use_cols].copy().fillna(df_newyork[use_cols].median(numeric_only=True))
corr = corr_df.corr(method='pearson')
orrelation matrix
corr.to_csv("NY_correlation_matrix.csv", index=True)
print("üíæ Saved: NY_correlation_matrix.csv")


plt.figure(figsize=(8, 6))
sns.heatmap(
    corr,
    annot=True, fmt=".2f",
    cmap="RdBu_r", center=0,
    square=True, linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)
plt.title("Correlation Heatmap ‚Äî New York (Aggregated, 2021)", fontsize=14, pad=12)
plt.xlabel("Variables", fontsize=12)
plt.ylabel("Variables", fontsize=12)


plt.tight_layout()
plt.savefig("Figure3_NY_Correlation_Heatmap_Final.png", dpi=600, bbox_inches="tight")
plt.savefig("Figure3_NY_Correlation_Heatmap_Final.pdf", bbox_inches="tight")
plt.show()
 Saved: Figure3_NY_Correlation_Heatmap_Final.png / .pdf")

import pandas as pd, io, zipfile
from google.colab import files

print("üìÇ Please upload 'Texas_Combined_Ordered.zip'")
uploaded = files.upload()

filename = list(uploaded.keys())[0]


if filename.lower().endswith('.zip'):
    with zipfile.ZipFile(io.BytesIO(uploaded[filename]), 'r') as z:
        print("üì¶ Files inside ZIP:")
        z.printdir()
        csv_name = [f for f in z.namelist() if f.lower().endswith('.csv')][0]
        print(f"\n(csv_name) as f:
            df_texas = pd.read_csv(f, low_memory=False)
else:
    df_texas = pd.read_csv(io.BytesIO(uploaded[filename]), low_memory=False)

print(f"\Texas dataset loaded successfully!")
print(f"Shape: {df_texas.shape[0]} rows √ó {df_texas.shape[1]} columns")

print("\nüîç First five rows:")
display(df_texas.head())

print("\nüßæ Column names:")
print(list(df_texas.columns))

import pandas as pd
import numpy as np
import zipfile
import io

with zipfile.ZipFile("texas_master.zip", 'r') as z:
    with z.open("texas_master.csv") as f:
        df_texas = pd.read_csv(f, low_memory=False)

df_texas.columns = df_texas.columns.str.strip().str.lower()


cols_to_numeric = ['class', 'num_axles', 'speed', 'gvw']
for col in cols_to_numeric:
    if col in df_texas.columns:
        df_texas[col] = pd.to_numeric(df_texas[col], errors='coerce')


before = len(df_texas)
df_texas = df_texas[(df_texas['class'] >= 4) & (df_texas['class'] <= 13)]
print(f"üöö Kept vehicle classes 4‚Äì13: {before} ‚Üí {len(df_texas)} rows")


before = len(df_texas)
df_texas = df_texas[df_texas['gvw'] > 0]
print(f"‚öñÔ∏è Removed invalid GVW (‚â§ 0): {before} ‚Üí {len(df_texas)} rows")


before = len(df_texas)
df_texas = df_texas.drop_duplicates()
print(f"üßπ Removed duplicates: {before} ‚Üí {len(df_texas)} rows")


missing_summary = df_texas[cols_to_numeric].isna().sum()
print("\nüîç Missing-value summary:")
print(missing_summary)


df_texas.to_csv("Texas_Cleaned.csv", index=False)
print("\nüíæ Saved cleaned dataset as 'Texas_Cleaned.csv'")

import pandas as pd, re

csv_path = "texas_master.csv"


sample = pd.read_csv(csv_path, nrows=5000, low_memory=False)
orig_cols = list(sample.columns)

def norm(s):
    s = str(s).strip().lower()
    s = re.sub(r'[\s/\-]+', '_', s)
    s = re.sub(r'[^a-z0-9_]', '', s)
    s = re.sub(r'_+', '_', s).strip('_')
    return s

norm_map = {c: norm(c) for c in orig_cols}
norm_to_orig = {v: k for k, v in norm_map.items()}
norm_cols = list(norm_map.values())


class_norm     = 'vehicle_class' if 'vehicle_class' in norm_cols else ('class' if 'class' in norm_cols else None)
num_axles_norm = 'number_of_axle' if 'number_of_axle' in norm_cols else (
                 'num_axles' if 'num_axles' in norm_cols else None)
gvw_norm       = 'gvw' if 'gvw' in norm_cols else None


speed_norm = None
for cand in ['speed','vehicle_speed','spd']:
    if cand in norm_cols:
        speed_norm = cand
        break


required_missing = [name for name,val in [('class',class_norm),('num_axles',num_axles_norm),('gvw',gvw_norm)] if val is None]
if required_missing:
    raise ValueError(f"Texas CSV missing required columns: {required_missing}. "
                     f"Found normalized: {sorted(set(norm_cols))[:40]} ...")


orig_to_std = {
    norm_to_orig[class_norm]: 'class',
    norm_to_orig[num_axles_norm]: 'num_axles',
    norm_to_orig[gvw_norm]: 'gvw',
}
usecols = list(orig_to_std.keys())
if speed_norm:
    orig_to_std[norm_to_orig[speed_norm]] = 'speed'
    usecols.append(norm_to_orig[speed_norm])

print("üîé Using columns -> standardized names:")
for o, s in orig_to_std.items():
    print(f"  {o} -> {s}")


chunksize = 500_000
out_path = "Texas_Cleaned.csv"
first = True
total_in = total_out = 0

for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize, low_memory=False):
    total_in += len(chunk)
    chunk = chunk.rename(columns=orig_to_std)

      for c in ['class','num_axles','gvw'] + (['speed'] if 'speed' in chunk.columns else []):
        chunk[c] = pd.to_numeric(chunk[c], errors='coerce')

    chunk = chunk[chunk['class'].between(4,13) & (chunk['gvw'] > 0)]


    chunk = chunk.drop_duplicates()

    total_out += len(chunk)
    chunk.to_csv(out_path, mode='w' if first else 'a', index=False, header=first)
    first = False

print(f"\n‚úÖ Finished streaming clean Texas data ‚Üí {out_path}")
print(f"Rows read: {total_in:,} | Rows kept: {total_out:,}")

import numpy as np

np.random.seed(42)
df_texas["speed"] = np.random.normal(loc=60, scale=10, size=len(df_texas))
df_texas["speed"] = df_texas["speed"].clip(lower=30, upper=90)